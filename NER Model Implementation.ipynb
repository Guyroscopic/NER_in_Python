{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing Required Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "from ner_utils.Sentence         import Sentence\n",
    "from ner_utils.Data             import Data\n",
    "from ner_utils.DataInformation  import DataInformation\n",
    "from ner_utils.DataPreparer     import DataPreparer\n",
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models        import Model\n",
    "from tensorflow.keras.layers        import Input, Conv2D, MaxPooling2D, Dropout, Dense, LSTM, Embedding, Concatenate, Reshape, Permute, Lambda, Bidirectional\n",
    "from tensorflow.keras.optimizers    import SGD\n",
    "\n",
    "from tensorflow_addons.layers import CRF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calulating Data Information \n",
    "#### This information will be used in defining hyper parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path     = 'data/ner_datasetreference.csv'\n",
    "dataset_encoding = 'latin'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_info = DataInformation(dataset_path, dataset_encoding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_len_info       = data_info.get_word_length_info()\n",
    "sentence_len_info   = data_info.get_sentence_length_info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining Initial Hyper Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Word Level Hyper Parameters\n",
    "model_word_len     = int(np.ceil(word_len_info['avg'] + 2*word_len_info['std']))\n",
    "\n",
    "\n",
    "#Sentence Level Hyper Parameters\n",
    "model_sentence_len = int(np.ceil(sentence_len_info['avg'] + 2*sentence_len_info['std']))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing Data to Input to Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_preparer = DataPreparer(dataset_path, dataset_encoding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_inputs, word_to_idx = data_preparer.get_tokenized_sentences(model_sentence_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_inputs, char_to_idx = data_preparer.get_tokenized_words(model_word_len, model_sentence_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# idx_to_word = { idx: word for word,idx in word_to_idx.items() }\n",
    "\n",
    "# sentence = []\n",
    "# for idx in sentence_inputs[8080]:\n",
    "#     sentence.append(idx_to_word[idx])\n",
    "\n",
    "# \" \".join(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# idx_to_char = { idx: char for char, idx in char_to_idx.items() }\n",
    "\n",
    "# sentence = []\n",
    "\n",
    "# for temp in word_inputs[8080]:\n",
    "    \n",
    "#     word = []\n",
    "#     for idx in temp:\n",
    "#         word.append(idx_to_char[idx])\n",
    "\n",
    "#     sentence.append(\"\".join(word))\n",
    "\n",
    "# \" \".join(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tags"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining Remaining Hyper Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11, 38, 31820)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Word Level Hyper Parameters\n",
    "char_embedding_dim = 35\n",
    "char_vocab_size    = len(char_to_idx) + 1 #add 1 for padding token\n",
    "window_size        = 3\n",
    "conv_filters       = 30 \n",
    "\n",
    "\n",
    "#Sentence Level Hyper Parameters\n",
    "word_embedding_dim = 100\n",
    "word_vocab_size    = len(word_to_idx) + 1 #add 1 for padding token\n",
    "lstm_units         = 300\n",
    "\n",
    "#Other Hyper Pararmeters\n",
    "dropout_rate    = 0.5\n",
    "n_tags          = 17#len(tags) \n",
    "initial_lr      = 0.015\n",
    "momentum        = 0.9\n",
    "decay_rate      = 0.05\n",
    "grad_clip       = 5\n",
    "\n",
    "model_word_len, model_sentence_len, word_vocab_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = Data('data/ner_datasetreference.csv', encoding='latin1')\n",
    "sentence = Sentence(data.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['n', 'm', 'v', 'B', '-', 'g', 'p', 'I', 't', 'i', 'o', 'e', 'a', 'O', 'r']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tags = sentence.tags\n",
    "tags"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting char voacb\n",
    "# with open('unique_chars.pkl', 'wb') as f:\n",
    "#     pickle.dump(sentence.get_unique_chars(), f)\n",
    "\n",
    "# loading char vocab   \n",
    "with open('unique_chars.pkl', 'rb') as f:\n",
    "    chars = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4.773359082564433, 2.8246976648249014, 64, 1)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Word info to pass to the model\n",
    "max_word_len, min_word_len, avg_word_len, std_word_len = sentence.get_word_info()\n",
    "avg_word_len, std_word_len,  max_word_len, min_word_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(104, 1, 21.863987989741236, 7.963596820721575)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sentence info to pass to the model\n",
    "max_sentence_len, min_sentence_len, avg_sentence_len, std_sentence_len = sentence.get_sentence_info()\n",
    "max_sentence_len, min_sentence_len, avg_sentence_len, std_sentence_len"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_list, sentence_list = sentence.get_words_and_sentences()\n",
    "word_sequences, sentence_sequences = data.get_tokenized_sequences(word_list, sentence_list, max_word_len, max_sentence_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Thousands', 'of', 'demonstrators', ..., 'to', 'the', 'attack'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "char_to_idx, word_to_idx, idx_to_char, idx_to_word = data.get_tokens(word_list, sentence_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_embedding_matrix_from_file(embedding_filename, vocab_size, embedding_dim, key_to_idx):\n",
    "    \n",
    "    with open(embedding_filename, 'r', encoding='latin') as f:\n",
    "        # char_embeddings = pickle.load(f)\n",
    "        \n",
    "        embedding_matrix = np.zeros((vocab_size, embedding_dim))\n",
    "        for line in f.readlines():\n",
    "            try:      \n",
    "                embedding_matrix[key_to_idx[line.split()[0]]] = np.array(line.split()[1:], dtype=np.float32)\n",
    "            except KeyError: pass\n",
    "            except ValueError: pass\n",
    "                \n",
    "    return embedding_matrix  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "char_embedding_matrix = generate_embedding_matrix_from_file('char embeddings/char_embeddings_with_features.txt', char_vocab_size, char_embedding_dim, char_to_idx)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "word_embedding_martrix = generate_embedding_matrix_from_file('../glove.6B/glove.6B.100d.txt', word_vocab_size, word_embedding_dim, word_to_idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Input and Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# word and sentence inputs\n",
    "word_inputs     = word_sequences\n",
    "sentence_inputs = sentence_sequences\n",
    "\n",
    "# target labels\n",
    "y = sentence.get_labels()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Abdul Rafey\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\numpy\\core\\_asarray.py:83: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  return array(a, dtype, copy=False, order=order)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(47959,)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(word_sequences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyper Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11, 38, 31819)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Word Part of Model\n",
    "char_inputs = Input(shape=(model_sentence_len, model_word_len))\n",
    "x           = Lambda(lambda x: tf.reshape(x, (model_sentence_len, model_word_len)))(char_inputs)\n",
    "x           = Embedding(char_vocab_size,\n",
    "                            char_embedding_dim,\n",
    "                            weights=[char_embedding_matrix],\n",
    "                            input_length=word_len_info['max'],\n",
    "                            trainable=True)(x)\n",
    "x           = Permute((2, 1), input_shape=(model_word_len, char_embedding_dim))(x)\n",
    "x           = Dropout(dropout_rate)(x)\n",
    "x           = Reshape((char_embedding_dim, model_word_len, 1))(x)\n",
    "x           = Conv2D(conv_filters, input_shape=(model_sentence_len, char_embedding_dim, model_word_len, 1), kernel_size=(1, window_size), padding='same')(x)\n",
    "x           = MaxPooling2D(pool_size=(1, model_word_len))(x)\n",
    "x           = Dense(1, input_shape=(char_embedding_dim, 1, conv_filters))(x)\n",
    "x           = Reshape((char_embedding_dim, ))(x)\n",
    "x           = Lambda(lambda x: tf.expand_dims(x, axis=0))(x)\n",
    "\n",
    "\n",
    "#Sentence Part of Model\n",
    "word_inputs = Input(shape=(model_sentence_len, ))\n",
    "y           = Embedding(word_vocab_size,\n",
    "                            word_embedding_dim,\n",
    "                            weights=[word_embedding_martrix],\n",
    "                            input_length=sentence_len_info['max'],\n",
    "                            trainable=False)(word_inputs)\n",
    "\n",
    "\n",
    "#Joining the two parts\n",
    "z = Concatenate(axis=2)([y, x])\n",
    "z = Dropout(dropout_rate)(z)\n",
    "z = Bidirectional(LSTM(lstm_units, return_sequences=True, recurrent_dropout=dropout_rate))(z)\n",
    "z = Dropout(dropout_rate)(z)\n",
    "z = CRF(n_tags)(z)\n",
    "outputs, _,  _, _ = z\n",
    "# outputs = z\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(10)\n",
    "model = Model(inputs=[char_inputs, word_inputs], outputs=outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_6 (InputLayer)            [(None, 38, 11)]     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lambda_5 (Lambda)               (38, 11)             0           input_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "embedding_3 (Embedding)         (38, 11, 35)         3535        lambda_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "permute_2 (Permute)             (38, 35, 11)         0           embedding_3[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dropout_4 (Dropout)             (38, 35, 11)         0           permute_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "reshape_4 (Reshape)             (38, 35, 11, 1)      0           dropout_4[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_2 (Conv2D)               (38, 35, 11, 30)     120         reshape_4[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2D)  (38, 35, 1, 30)      0           conv2d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (38, 35, 1, 1)       31          max_pooling2d_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "input_7 (InputLayer)            [(None, 38)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "reshape_5 (Reshape)             (38, 35)             0           dense_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "embedding_4 (Embedding)         (None, 38, 100)      3182000     input_7[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lambda_6 (Lambda)               (1, 38, 35)          0           reshape_5[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (1, 38, 135)         0           embedding_4[0][0]                \n",
      "                                                                 lambda_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dropout_5 (Dropout)             (1, 38, 135)         0           concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_1 (Bidirectional) (1, 38, 600)         1046400     dropout_5[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_6 (Dropout)             (1, 38, 600)         0           bidirectional_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "crf_1 (CRF)                     [(1, 38), (1, 38, 17 10540       dropout_6[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 4,242,626\n",
      "Trainable params: 1,060,626\n",
      "Non-trainable params: 3,182,000\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lr_scheduler(epoch, lr): return initial_lr / (1 + decay_rate * epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt         = SGD(learning_rate=initial_lr, momentum=0.9, clipvalue=grad_clip)\n",
    "callback    = tf.keras.callbacks.LearningRateScheduler(lr_scheduler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=opt, loss=\"mse\", metrics=[\"mae\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 2,  7, 16,  0,  2,  7, 16,  0,  2,  7, 16,  0,  2,  7, 16,  0,\n",
       "         2,  7, 16,  0,  2,  7, 16,  0,  2,  7, 16,  0,  2,  7, 16,  0,\n",
       "         2,  7, 16,  0,  2,  7]])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# char_inputs = np.random.uniform(size=(1, 38, 11))\n",
    "# word_inputs = np.random.uniform(size=(1, 38,   ))\n",
    "\n",
    "\n",
    "\n",
    "# out = model.predict([char_inputs, word_inputs])\n",
    "\n",
    "# out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# history = model.fit([word_sequences, sentence_sequences], y, callbacks=[callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "38"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(out[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "90f3c6dc1a635370bb41746b2e6bfa7ebff435df73366e599fe7a07aa00d9a9c"
  },
  "kernelspec": {
   "display_name": "Python 3.7.4 64-bit",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
