{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/ner_datasetreference.csv', encoding='latin').fillna(method='ffill')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.preprocessing.text import text_to_word_sequence, Tokenizer\n",
    "\n",
    "dataset = df.copy()\n",
    "\n",
    "\n",
    "def _get_word_tokens():\n",
    "        \"\"\"\n",
    "        Private method to generate word tokens.\n",
    "\n",
    "        @return\n",
    "        A tuple of dictionaries, first maps words to integer tokens, \n",
    "        second maps integer tokens to words\n",
    "        \"\"\"\n",
    "\n",
    "        sentences = dataset.groupby(['Sentence #'])['Word'].transform(lambda word : ' '.join(word)).drop_duplicates()\n",
    "\n",
    "        tokenizer = Tokenizer(filters=\"\", lower=True, oov_token='<UNK>', char_level=False)\n",
    "        tokenizer.fit_on_texts(list(sentences))\n",
    "\n",
    "        return tokenizer.word_index\n",
    "\n",
    "\n",
    "def get_tokenized_sentences(max_sentence_len):\n",
    "        \"\"\"\n",
    "        Public method for ...\n",
    "\n",
    "        @return\n",
    "        \"\"\"\n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "        word_to_idx          = _get_word_tokens()\n",
    "        word_to_idx['<PAD>'] = 0\n",
    "\n",
    "        temp_dataset               = dataset.copy()\n",
    "        temp_dataset['word_token'] = temp_dataset['Word'].str.lower().map(word_to_idx)           \n",
    "        tokenized_sentences        = temp_dataset.groupby(['Sentence #'])['word_token'].apply(np.array)\n",
    "        padded_tokenized_sentences = pad_sequences(tokenized_sentences, maxlen=max_sentence_len, value=0, padding='post', truncating='post') \n",
    "\n",
    "        return padded_tokenized_sentences, word_to_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_inputs, word_to_idx = get_tokenized_sentences(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx_to_word = {idx: word for word, idx in word_to_idx.items() }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'thousands of demonstrators have marched through london to protest the war in iraq and demand the withdrawal of british troops from that country . <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> '"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "string = \"\"\n",
    "\n",
    "for i in sentence_inputs[0]:\n",
    "    string += idx_to_word[i] + \" \"\n",
    "\n",
    "string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(47959, 30)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_inputs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 259,    6,  974,   16, 1791,  237,  467,    7,  522,    2,  129,\n",
       "          5,   61,    9,  575,    2,  832,    6,  185,   90,   22,   15,\n",
       "         56,    3,    0,    0,    0,    0,    0,    0])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_inputs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.preprocessing.text import text_to_word_sequence, Tokenizer\n",
    "\n",
    "dataset = df.copy()\n",
    "\n",
    "\n",
    "def _get_char_tokens():\n",
    "        \"\"\"\n",
    "        Private method to generate character tokens.\n",
    "\n",
    "        @return\n",
    "        A tuple of dictionaries, first maps chars words to integer tokens, \n",
    "        second maps integer tokens to chars\n",
    "        \"\"\"\n",
    "\n",
    "        words     = dataset['Word'].values\n",
    "\n",
    "        tokenizer = Tokenizer(lower=False, oov_token='<UNK>', char_level=True)\n",
    "        tokenizer.fit_on_texts(list(words))\n",
    "\n",
    "        return tokenizer.word_index\n",
    "\n",
    "\n",
    "\n",
    "def get_tokenized_words(max_word_len, max_sentence_len):\n",
    "        \"\"\"\n",
    "        Public method for ...\n",
    "\n",
    "        @return\n",
    "        \"\"\"\n",
    "\n",
    "        pad_value = 0\n",
    "\n",
    "        char_to_idx          = _get_char_tokens()\n",
    "        char_to_idx['<PAD>'] = pad_value\n",
    "\n",
    "        temp_dataset                = dataset.copy()\n",
    "        temp_dataset['char_tokens'] = dataset['Word'].str.split(\"\").str[1:-1]\n",
    "        temp_dataset['char_tokens'] = temp_dataset['char_tokens'].apply(lambda x: [char_to_idx[i] for i in x])\n",
    "        padded_tokenized_words      = temp_dataset.groupby(['Sentence #'])['char_tokens'].apply(np.array).apply(pad_sequences, maxlen=max_word_len, value=pad_value , padding='post', truncating='post')\n",
    "        padded_tokenized_words      = pad_sequences(padded_tokenized_words, maxlen=max_sentence_len, value=[pad_value]*max_word_len , padding='post', truncating='post')\n",
    "\n",
    "        return padded_tokenized_words, char_to_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "pop, char_to_idx = get_tokenized_words(5, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(47959, 10, 5)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pop.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[27, 10,  2,  0,  0],\n",
       "       [37,  9,  5,  4,  5],\n",
       "       [ 3,  6, 12,  0,  0],\n",
       "       [30,  9,  5,  8, 10],\n",
       "       [17,  9,  5, 15,  2],\n",
       "       [15,  5,  6,  5,  8],\n",
       "       [ 8,  3, 19,  0,  0],\n",
       "       [15, 14, 13, 10,  0],\n",
       "       [10,  3,  8,  0,  0],\n",
       "       [22,  2,  2,  6,  0]])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pop[9999] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "<class 'list'>\n"
     ]
    }
   ],
   "source": [
    "model_word_len = 10\n",
    "model_sentence_len = 30\n",
    "\n",
    "empty_word = [0] * model_word_len\n",
    "\n",
    "padded_sentences = []\n",
    "\n",
    "for sentence in pop[:2]:\n",
    "    # print(type(sentence))\n",
    "    sentence = list(pad_sequences(sentence, maxlen=model_word_len, padding='post', truncating='post'))\n",
    "\n",
    "    while len(sentence) < model_sentence_len:\n",
    "        sentence.append(empty_word)\n",
    "\n",
    "    padded_sentences.append(sentence)\n",
    "\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[27, 10,  7, 14,  8,  3,  6, 12,  8,  0],\n",
       "       [ 7, 16,  0,  0,  0,  0,  0,  0,  0,  0],\n",
       "       [12,  2, 15,  7,  6,  8,  4,  9,  3,  4],\n",
       "       [10,  3, 23,  2,  0,  0,  0,  0,  0,  0],\n",
       "       [15,  3,  9, 13, 10,  2, 12,  0,  0,  0],\n",
       "       [ 4, 10,  9,  7, 14, 18, 10,  0,  0,  0],\n",
       "       [55,  7,  6, 12,  7,  6,  0,  0,  0,  0],\n",
       "       [ 4,  7,  0,  0,  0,  0,  0,  0,  0,  0],\n",
       "       [17,  9,  7,  4,  2,  8,  4,  0,  0,  0],\n",
       "       [ 4, 10,  2,  0,  0,  0,  0,  0,  0,  0],\n",
       "       [21,  3,  9,  0,  0,  0,  0,  0,  0,  0],\n",
       "       [ 5,  6,  0,  0,  0,  0,  0,  0,  0,  0],\n",
       "       [30,  9,  3, 44,  0,  0,  0,  0,  0,  0],\n",
       "       [ 3,  6, 12,  0,  0,  0,  0,  0,  0,  0],\n",
       "       [12,  2, 15,  3,  6, 12,  0,  0,  0,  0],\n",
       "       [ 4, 10,  2,  0,  0,  0,  0,  0,  0,  0],\n",
       "       [21,  5,  4, 10, 12,  9,  3, 21,  3, 11],\n",
       "       [ 7, 16,  0,  0,  0,  0,  0,  0,  0,  0],\n",
       "       [37,  9,  5,  4,  5,  8, 10,  0,  0,  0],\n",
       "       [ 4,  9,  7,  7, 17,  8,  0,  0,  0,  0],\n",
       "       [16,  9,  7, 15,  0,  0,  0,  0,  0,  0],\n",
       "       [ 4, 10,  3,  4,  0,  0,  0,  0,  0,  0],\n",
       "       [13,  7, 14,  6,  4,  9, 19,  0,  0,  0],\n",
       "       [20,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
       "       [ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
       "       [ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
       "       [ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
       "       [ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
       "       [ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
       "       [ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0]])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(padded_sentences[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = df.copy()\n",
    "\n",
    "def _get_unique_tags():\n",
    "\n",
    "        return df['Tag'].unique()\n",
    "\n",
    "def get_tags(max_sentence_len):\n",
    "        \"\"\"\n",
    "        Public method for ...\n",
    "\n",
    "        @return\n",
    "        \"\"\"\n",
    "\n",
    "        unique_tags = _get_unique_tags()\n",
    "        tag_to_idx  = { tag: idx+1 for idx, tag in enumerate(unique_tags) } #0 index is for no tag\n",
    "\n",
    "        temp_dataset = dataset.copy()\n",
    "\n",
    "\n",
    "        temp_dataset['tag_token'] = temp_dataset['Tag'].map(tag_to_idx)                   \n",
    "        tokenized_tags            = temp_dataset.groupby(['Sentence #'])['tag_token'].apply(np.array)\n",
    "        padded_tokenized_tags     = pad_sequences(tokenized_tags, maxlen=max_sentence_len, value=0, padding='post', truncating='post') \n",
    "\n",
    "\n",
    "        return padded_tokenized_tags, tag_to_idx\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "tags, tag_to_idx = get_tags(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 2, 1, 1])"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tags[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'O': 1,\n",
       " 'B-geo': 2,\n",
       " 'B-gpe': 3,\n",
       " 'B-per': 4,\n",
       " 'I-geo': 5,\n",
       " 'B-org': 6,\n",
       " 'I-org': 7,\n",
       " 'B-tim': 8,\n",
       " 'B-art': 9,\n",
       " 'I-art': 10,\n",
       " 'I-per': 11,\n",
       " 'I-gpe': 12,\n",
       " 'I-tim': 13,\n",
       " 'B-nat': 14,\n",
       " 'B-eve': 15,\n",
       " 'I-eve': 16,\n",
       " 'I-nat': 17}"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tag_to_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "db3ca9ec755843094bfe658d337cf97d381b019c52f517307d44cf002f9583d1"
  },
  "kernelspec": {
   "display_name": "Python 3.8.3 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
